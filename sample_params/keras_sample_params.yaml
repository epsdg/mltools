
train_mode: minibatch_gd  # sgd, minibatch_gd
epoch_train_batches: 5  # sgd only, ignored for minibatch_gd
epoch_val_batches: 5  # sgd only, ignored for minibatch_gd

n_epochs: 42

loss_fn: mean_squared_error
# mean_squared_error, mean_absolute_error, binary_crossentropy, ...

optimizer: sgd  # sgd, rmsprop, adagrad, adadelta, adam, adamax, nadam
train_batch_size: 500
eta: 0.003
decay: 0.1
beta_1: 0.9
beta_2: 0.999
epsilon: 1e-8
momentum: 0.999
amsgrad: False  # for optimizer='adam' only

reduce_eta: False  # do not use with find_eta or use_clr
reduce_eta_steps: 5
reduce_eta_factor: 0.2
early_stop_rounds: 0

find_eta: False # do not use with reduce_eta or use_clr
find_eta_start: 1e-06 # recommend 1e-07 - 1e-06
find_eta_factor: 1.2  # recommend ~ 1.1 (fine-tuning) - 1.4 (fast)

use_clr: True  # do not use with find_eta or reduce_eta
clr_min: 0.0003
clr_max: 0.04
clr_period: 8 # even integer >= 4, recommend (n_epochs % clr_period) = 0

val_batch_size: 500
verbose: 0
use_tensorboard: True

init_mode: fan_avg    #  fan_in, fan_out, fan_avg
init_distribution: normal
layers: [512, 512, 512, 512, 512]
activation: elu   #  relu, elu, selu, crelu, tanh
regularizer: l1-l2   # l1, l2, l1-l2, None
drop_rates: [0.2, 0.2, 0.2, 0.2, 0.2]
l2_reg_weight: 5e-7
l1_reg_weight: 2e-7
use_batch_norm: [True, True, True, True, True]
batch_norm_momentum: 0.9999
pos_weight: 1 # ignored for regression

predict_chunk_size: 1000
binary_metrics: keras  # keras, sklearn, both
